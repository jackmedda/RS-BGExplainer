# choices ["DPBG", "BaB"]
explainer: DPBG
# optimizer for explainer
cf_optimizer: SGD
# learning rate of optimizer for explainer
cf_learning_rate: 3000 # 3000 (deletion) / 65000 (addition) for NDCGApprox, 300 (deletion) for SoftmaxLoss
# set to update weights after each mini-batch, otherwise after a full batch
mini_batch_descent: True

momentum: 0.0
# epochs to generate explanations
cf_epochs: 800
# how many users should be considered to explain?
user_batch_exp: 64
# not consider the prediction loss
not_pred: True
# if the fair or pred loss should be deactivated if the top-k list is already perturbed
pred_same: False
# loss weight for the graph dist loss
cf_beta: 0.01  # 0.01 for NDCGApproxLoss, 4.0 for SoftmaxLoss
# the function used to check if the top-k list was perturbed
cf_dist: damerau_levenshtein
# how many top items to consider for model outcome list and fairness loss
cf_topk: 10

# metric used for demographic parity loss and debugging. choices ["ndcg", "softmax"]
metric_loss: ndcg

# metric to evaluate the performance of the model. choices ["ndcg", "hit", "recall", "mrr", "precision"]
eval_metric: ndcg

# metric to evaluate the fairness of the model. choices ["DP_across_random_samples", "DP"]
fair_metric: DP

dropout_prob: 0.3

edge_additions: False

# select which set need to be used to optimize the DP explainer. Choices ["rec", "train", "valid", "test"]
exp_rec_data: "test"

# device: cpu

# "local": the optimization uses the NDCG of the disadvantaged group computed locally for the current batch
# "global": the optimization uses the NDCG of the disadvantaged group computed globally on the original model
only_adv_group: "local"
# Choose if the edges of the advantaged group should be perturbed or the opposite
perturb_adv_group: True

# Last.FM 1K
# data_path: 'src/dataset/lastfm-1k' # for Last.FM 1K
# ML-1M
# data_path: 'dataset/ml-1m' # for ML-1M

# consider only the nodes at the last hop
sub_matrix_only_last_level: False
# does not consider the interactions of the user to explain, consider the remaining hops
not_user_sub_matrix: False
# does not perform perturbation, uses only the subgraph of the user to `explain` (used for analysis)
only_subgraph: False

# sensitive attributes to be used in the fairness losses and for following analysis
sensitive_attribute: age

# if True each new exp will have a different number of edges from the last one
save_unique_graph_dist_loss: True

# force return of explanations even though the top-k is not perturbed (only useful for hops analysis)
explainer_force_return: False

# ML-100K
load_col:
    inter: [user_id, item_id, timestamp]
    item: [item_id, class]
    user: [user_id, gender, age]

eval_args:
    split: {'LRS': None}
    order: RO  # not relevant
    group_by: '-'
    mode: 'full'

# Early stopping parameters
early_stopping:
    patience: 40  # a periodicity is visible on the charts, with low peaks better than the previous ones after 20-40 epochs
    ignore: 0
    method: 'consistency'
    mode: 'lt'  # lower than, use any other python func ('le', 'gt', 'ge')
    delta: 0.0005  # several experiments show many low peaks with differences about > 0.15, which are relevant for us
    check_value: 'fair_metric'  # ['fair_loss', 'fair_metric']

previous_loss_value: False
previous_batch_LR_scaling: False

# use 'random' to initialize it randomly around a fixed value, otherwise the fixed value will be used
perturbation_initialization: 'static'

# Policies
explainer_policies:
    increase_disparity: False
    force_removed_edges: True
    group_deletion_constraint: False   # this and `random_perturbation` cannot be both True
    random_perturbation: False
    neighborhood_perturbation: False   # the perturbation spread from the first perturbed edges towards the neighbors
    users_zero_constraint: False  # only perturbs users with `eval_metric` <= `users_zero_constraint_value`

random_perturbation_p: 0.0001
users_zero_constraint_value: 0
